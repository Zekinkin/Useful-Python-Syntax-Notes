{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Removing HTML Tags and Special Characters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    clean_text = re.sub(r'<.*?>', '', text)\n",
    "    return clean_text\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    clean_text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Tokenization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization breaks down text into its constituent parts and facilitates the counting and analysis of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Be carefull : ```word_tokenize(text)``` ≠ ```text.split()```**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\zekin\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'a', 'good', 'good', 'boy']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"I am a good good boy\"\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "print(tokenize_text(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Lowercasing & Uppercasing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_lowercase(text):\n",
    "    lowercased_text = text.lower()\n",
    "    return lowercased_text\n",
    "\n",
    "def convert_to_lowercase(text):\n",
    "    lowercased_text = text.upper()\n",
    "    return lowercased_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Stopword Removal**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords are common words such as “the,” “and,” or “in” that carry little meaningful information in many NLP tasks. Removing stopwords can reduce noise and improve the efficiency of text analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**```remove_stopwords``` expects tokens (a list), if you're passing text (a string). You need to tokenize it first.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\zekin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = tokenize_text(\"I am a good good boy and the cool guy or you can call me zzippy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'good', 'good', 'boy', 'cool', 'guy', 'call', 'zzippy']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_tokens = remove_stopwords(text)\n",
    "filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. Stemming and Lemmatization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming and lemmatization are techniques to reduce words to their root forms, which can help group similar words. Stemming is more aggressive and may result in non-dictionary words, whereas lemmatization produces valid words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\zekin\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def stem_text(tokens):\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
    "    return stemmed_tokens\n",
    "\n",
    "def lemmatize_text(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = word_tokenize(\"I am running and I used to love swimming, I love you and you loved me!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'am',\n",
       " 'run',\n",
       " 'and',\n",
       " 'i',\n",
       " 'use',\n",
       " 'to',\n",
       " 'love',\n",
       " 'swim',\n",
       " ',',\n",
       " 'i',\n",
       " 'love',\n",
       " 'you',\n",
       " 'and',\n",
       " 'you',\n",
       " 'love',\n",
       " 'me',\n",
       " '!']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_text(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'am',\n",
       " 'running',\n",
       " 'and',\n",
       " 'I',\n",
       " 'used',\n",
       " 'to',\n",
       " 'love',\n",
       " 'swimming',\n",
       " ',',\n",
       " 'I',\n",
       " 'love',\n",
       " 'you',\n",
       " 'and',\n",
       " 'you',\n",
       " 'love',\n",
       " 'me',\n",
       " '!']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize_text(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6.Removing Duplicate Text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(token):\n",
    "    unique_texts = list(set(token))\n",
    "    return unique_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " 'you',\n",
       " 'am',\n",
       " 'love',\n",
       " 'running',\n",
       " ',',\n",
       " 'swimming',\n",
       " 'used',\n",
       " 'to',\n",
       " 'loved',\n",
       " 'and',\n",
       " 'me',\n",
       " 'I']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_2 = remove_duplicates(token)\n",
    "text_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **7.Token to String**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I amm a gud boy'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [\"I\", \"amm\", \"a\", \"gud\", \"boy\"]\n",
    "strings = \" \".join(tokens)\n",
    "strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **8.Dealing with Noisy Text**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noisy text data can include typos, abbreviations, non-standard language usage, and other irregularities. Addressing such noise is crucial for ensuring the accuracy of text analysis. Techniques like spell-checking, correction, and custom rules for specific noise patterns can be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "def correct_spelling(tokens):\n",
    "    spell = SpellChecker()\n",
    "    corrected_tokens = [spell.correction(word) for word in tokens]\n",
    "    corrected_text = ' '.join(corrected_tokens)\n",
    "    return corrected_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [\"I\", \"amm\", \"a\", \"gud\", \"boy\"]\n",
    "type(correct_spelling(tokens))  # Outputs: \"I am a good boy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **9. Handling Encoding Issues**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding problems can lead to unreadable characters or errors during text processing. Ensuring that text is correctly encoded (e.g., UTF-8) is crucial to prevent issues related to character encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`text.encode('utf-8').decode('utf-8')` converts a string to UTF-8 bytes and then back to a string. It’s a no-op for valid UTF-8 text—output equals input—but can raise errors if the text has invalid encoding, effectively validating it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_encoding(text):\n",
    "    try:\n",
    "        decoded_text = text.encode('utf-8').decode('utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        decoded_text = 'Encoding Error'\n",
    "    return decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am running and I used to love swimming, I love you and you loved me!'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I am running and I used to love swimming, I love you and you loved me!\"\n",
    "fix_encoding(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **10. Whitespace Removal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_whitespace(text):\n",
    "    cleaned_text = ' '.join(text.split())\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'am',\n",
       " 'running',\n",
       " 'and',\n",
       " 'I',\n",
       " 'used',\n",
       " 'to',\n",
       " 'love',\n",
       " 'swimming,',\n",
       " 'I',\n",
       " 'love',\n",
       " 'you',\n",
       " 'and',\n",
       " 'you',\n",
       " 'loved',\n",
       " 'me!']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I am running and I used to love swimming, I love you and you loved me!\"\n",
    "text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **11. Handling Text Language Identification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        language = detect(text)\n",
    "    except:\n",
    "        language = 'unknown'\n",
    "    return language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I am running and I used to love swimming, I love you and you loved me!\"\n",
    "detect_language(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ko'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"我喜欢你\"\n",
    "detect_language(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **12. Handling Text Length Variation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text data often varies in length, and extreme variations can affect the performance of text analysis algorithms. Depending on your analysis goals, you may need to normalize text length. Techniques include:\n",
    "\n",
    "Padding: Adding tokens to shorter text samples to make them equal in length to longer samples. This is commonly used in tasks like text classification, requiring fixed input lengths.\n",
    "Text Summarization: Reducing the length of longer texts by generating concise summaries can be useful for information retrieval or summarization tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def pad_text_sequences(text_sequences, max_length):\n",
    "    padded_sequences = pad_sequences(text_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sequences = [[\"I love eating ice cream\"],[\"I am cool\"],[\"I am running and I used to love swimming, I love you and you loved me!\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **13.Handling Biases and Fairness**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In text data, biases related to gender, race, or other sensitive attributes can be present. Addressing these biases is crucial for ensuring fairness in NLP applications. Techniques include debiasing word embeddings and using reweighted loss functions to account for bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debias_word_embeddings(embeddings, gender_specific_words):\n",
    "    # Implement a debiasing technique to reduce gender bias in word embeddings\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **14. Handling Large Text Corpora**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When dealing with large text corpora, memory and processing time become critical. Data streaming, batch processing, and parallelization can be applied to clean and process large volumes of text data efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```num_workers``` processes for parallel execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def parallel_process_text(data, cleaning_function, num_workers):\n",
    "    with Pool(num_workers) as pool:\n",
    "        cleaned_data = pool.map(cleaning_function, data)\n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```strip()```: 移除字符串两端的空白字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    return text.lower().strip()\n",
    "\n",
    "texts = [\"Hello World \", \"Python ROCKS\", \" Multiprocessing \"]\n",
    "result = parallel_process_text(texts, clean_text, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **15. Handling Multilingual Text Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text data can be multilingual, which adds a layer of complexity. Applying language-specific cleaning and preprocessing techniques is important when dealing with multilingual text. Libraries like spaCy and NLTK support multiple languages and can be used to tokenize, lemmatize, and clean text in various languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "def clean_multilingual_text(text, language_code):\n",
    "    # 加载指定语言模型\n",
    "    nlp = spacy.load(language_code)  \n",
    "    # 处理文本生成文档对象\n",
    "    doc = nlp(text)  \n",
    "    # 提取每个词的词干并用空格连接\n",
    "    cleaned_text = ' '.join([token.lemma_ for token in doc])  \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I be run and jump'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I am running and jumping\"\n",
    "result = clean_multilingual_text(text, \"en_core_web_sm\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **16. Handling Text Data with Long Documents**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Long documents, such as research papers or legal documents, can pose challenges in text analysis due to their length. Techniques like text summarization or document chunking can extract key information or break long documents into manageable sections for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c88e7b398d843b99543fd1401146ed1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4445366a0d94117a14bbe922215ae22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91fc1bb623494bf7807eb82ea074f924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "197722c0a5cc4ffe8f2e67c50cc62788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3104c82795a467291d6d8c17607e5b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6338880210e2474eb92ee2179cc227ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI.    ‘ ’   ‘’ ’’”’.  “”  ”” ‘. ” ”. “.  ” “’ ”.   “\"” \"”\"’\" \"“ ’A’: “I’m sorry.”\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 加载摘要模型\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 130, but your input_length is only 120. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=60)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experts predict the AI market will exceed $1 trillion by 2030. Public opinion is divided, seeing AI as both progress and risk. Its future holds potential and challenges, requiring a balance of innovation and responsibility.\n"
     ]
    }
   ],
   "source": [
    "# 输入文本\n",
    "text = \"Artificial intelligence has rapidly advanced, transforming industries like healthcare, finance, education, and transportation. Deep learning breakthroughs in image recognition and NLP have enabled self-driving cars and smart assistants. However, AI raises ethical and privacy issues, such as data misuse and bias. Experts predict the AI market will exceed $1 trillion by 2030, outpacing regulatory efforts. Automation boosts efficiency but causes job losses, while AI aids drug development and climate modeling. Public opinion is divided, seeing AI as both progress and risk. Its future holds potential and challenges, requiring a balance of innovation and responsibility.\"\n",
    "# 生成摘要\n",
    "# max_length=130: 生成摘要的最大长度（词或标记数），不超过130。\n",
    "# min_length=30: 生成摘要的最小长度（词或标记数），不少于30。\n",
    "# do_sample=False: 是否使用采样生成摘要，False表示用确定性方法（贪婪或束搜索），结果更稳定。\n",
    "summary = summarizer(text, max_length=130, min_length=30, do_sample=False)\n",
    "print(summary[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **17. Handling Text Data with Time References**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text data that includes time references, such as dates or timestamps, may require special handling. You can extract and standardize time-related information, convert it to a standard format, or use it to create time series data for temporal analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dates_and_times(text):\n",
    "    # Implement date and time extraction logic (e.g., using regular expressions)\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
